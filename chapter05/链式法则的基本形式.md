## 链式法则的基本形式

假设我们有一个复合函数 $$ y = f(g(x))$$，其中 $$f$$ 和 $$g$$ 都是可导函数，链式法则给出了 $$y$$ 对 $$x$$ 的导数：

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

这个公式可以应用到神经网络中。每一层的误差（残差）是前一层误差通过链式法则传播的结果。

## 神经网络中的链式法则

在神经网络中，假设有多层，每一层都有输入、权重、偏置、激活函数等。具体来说，神经网络每一层的输出 $$a^l$$ 和输入 $$z^l$$ 之间的关系如下：

$$z^l = W^l a^{l-1} + b^l$$

$$a^l = f(z^l)$$

- $$W^l$$ 是第 $$l$$ 层的权重矩阵，$$b^l$$ 是偏置，$$a^{l-1}$$ 是上一层的输出（对于输入层，$$a^0 = X$$）。
- $$f(z^l)$$ 是第 $$l$$ 层的激活函数。

## 从损失函数到参数的梯度计算

假设我们有一个损失函数 $$L$$，它依赖于网络的输出层 $$a^L$$ 和真实标签 $$y$$。目标是最小化损失函数，因此我们需要计算损失函数关于每个参数（权重 $$W^l$$ 和偏置 $$b^l$$）的梯度。

我们用链式法则计算这些梯度。

### 输出层的梯度

假设网络的损失函数是交叉熵损失，对于输出层的梯度（即残差），我们有：

$$\delta^L = \frac{\partial L}{\partial z^L} = \hat{y} - y$$

其中：
- $$\hat{y} = f(z^L)$$ 是输出层的预测结果，$$y$$ 是真实标签（独热编码）。

对于交叉熵损失函数 $$L = - \sum_i y_i \log(\hat{y}_i)$$，我们可以得到输出层的残差（误差）：

$$\delta^L = \hat{y} - y$$

### 隐含层的梯度

接下来，我们使用链式法则从输出层的梯度反向传播到隐藏层。对于第 $$l$$ 层的梯度，我们使用链式法则：

$$\delta^l = \frac{\partial L}{\partial z^l} = \left( \frac{\partial L}{\partial z^{l+1}} \right) \cdot \left( \frac{\partial z^{l+1}}{\partial a^l} \right) \cdot \left( \frac{\partial a^l}{\partial z^l} \right)$$

其中：
- $$\delta^{l+1}$$ 是第 $$l+1$$ 层的残差（误差），
- $$\frac{\partial z^{l+1}}{\partial a^l} = W^{l+1}$$ 是第 $$l+1$$ 层的权重，
- $$\frac{\partial a^l}{\partial z^l}$$ 是第 $$l$$ 层激活函数的导数。

具体的推导步骤是：
1. **第一项：** 误差从下一层传递过来，即 $$\delta^{l+1}$$。
2. **第二项：** 计算下一层的输出与当前层的输入之间的关系。我们知道：
   $$\frac{\partial z^{l+1}}{\partial a^l} = W^{l+1}$$
3. **第三项：** 计算当前层的激活函数的导数，即：
   $$\frac{\partial a^l}{\partial z^l} = f'(z^l)$$

这样，我们就可以利用链式法则递归地计算每一层的梯度，直到输入层为止。

### 梯度计算公式

1. **输出层的残差（误差）**：
   $$\delta^L = \hat{y} - y$$
2. **隐含层的残差（误差）**：
   $$\delta^l = (W^{l+1})^T \delta^{l+1} \odot f'(z^l)$$
   其中 $$\odot$$ 表示元素级的乘法。







## 假设

假设一个简单的前馈神经网络具有 $$L $$ 层，其中：
- $$X $$ 是输入数据（大小为 $$d \times n $$），
- $$W^l $$ 是第 $$l $$ 层的权重矩阵（大小为 $$m_l \times m_{l+1} $$），
- $$b^l $$ 是第 $$l $$ 层的偏置（大小为 $$m_{l+1} \times 1 $$），
- $$a^l $$ 是第 $$l $$ 层的激活输出，
- $$z^l = W^l a^{l-1} + b^l $$ 是第 $$l $$ 层的线性输入。

我们使用 **Sigmoid** 激活函数：
$$a^l = \sigma(z^l) = \frac{1}{1 + \exp(-z^l)}$$
其中，$$\sigma(z^l) $$ 表示 Sigmoid 激活函数。

### 输出层的残差（误差）

首先，我们需要计算输出层的残差（误差）。设 $$L $$ 为损失函数，$$y $$ 为目标标签，$$\hat{y} $$ 为网络输出，且输出层的激活为 $$a^L $$。对于交叉熵损失和 softmax 输出，输出层的残差为：

$$\delta^L = \frac{\partial L}{\partial z^L} = \hat{y} - y$$

其中，$$\hat{y} $$ 是网络的输出，$$y $$ 是真实标签。

### 隐藏层的残差（误差）

为了计算每一层的权重，我们需要通过反向传播计算每一层的残差。对于第 $$l $$ 层的误差，使用链式法则从输出层传递回去：

$$\delta^l = \frac{\partial L}{\partial z^l} = \left( \frac{\partial L}{\partial z^{l+1}} \right) \cdot \left( \frac{\partial z^{l+1}}{\partial a^l} \right) \cdot \left( \frac{\partial a^l}{\partial z^l} \right)$$

#### 1. 误差项 \(\frac{\partial L}{\partial z^{l+1}}$

这是上一层的残差 $$\delta^{l+1}$$，即：

$$\frac{\partial L}{\partial z^{l+1}} = \delta^{l+1}$$

#### 2. 线性输入到激活函数的导数 $\frac{\partial z^{l+1}}{\partial a^l}$

这是第 $$l+1 $$ 层的权重 $$W^{l+1} $$，即：

$$\frac{\partial z^{l+1}}{\partial a^l} = W^{l+1}$$

#### 3. 激活函数的导数 \(\frac{\partial a^l}{\partial z^l}$

对于 Sigmoid 激活函数，导数是：

\[
\frac{\partial a^l}{\partial z^l} = a^l (1 - a^l)
\]

将这些合起来，得到第 $$l $$ 层的残差：

$$\delta^l = \delta^{l+1} (W^{l+1})^T \cdot a^l (1 - a^l)$$

### 权重的梯度

权重 $$W^l $$ 的梯度可以通过链式法则计算。我们需要计算损失函数对 $$W^l $$ 的导数：

$$\frac{\partial L}{\partial W^l} = \frac{\partial L}{\partial z^l} \cdot \frac{\partial z^l}{\partial W^l}$$

1. 误差对 $$z^l $$ 的导数：$$\frac{\partial L}{\partial z^l} = \delta^l $$
2. 线性输入对权重 $$W^l $$ 的导数：$$\frac{\partial z^l}{\partial W^l} = a^{l-1} $$

因此，权重 $$W^l ​$$ 的梯度是：

$$\frac{\partial L}{\partial W^l} = \delta^l \cdot (a^{l-1})^T$$





